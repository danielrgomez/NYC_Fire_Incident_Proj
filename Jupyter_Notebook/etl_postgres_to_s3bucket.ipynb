{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e632b788-7561-4ede-9dbf-c13eb8bdcfc5",
   "metadata": {},
   "source": [
    "## Extract from Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13681c-5581-4efc-b239-844cd113523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68505c31-37c8-430f-89c1-c3311d8cfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_csv():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='fire_incidents_db',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        host='fire_incidents_db_container',\n",
    "        port=5432\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM fire_incidents_tbl\")\n",
    "    \n",
    "    with open('./temp_csv_files/exported_nyc_fire_incidents_data.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i[0] for i in cursor.description])  # Write headers\n",
    "        writer.writerows(cursor.fetchall())  # Write data\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "export_data_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6cec2-b50e-4c03-9081-7d33994d3481",
   "metadata": {},
   "source": [
    "## Upload CSV File to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc67ec-c4e6-4dde-a2d5-591f7f4d85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312268e-d9ff-459f-8837-117b595622cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3():\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id='your_access_key',\n",
    "        aws_secret_access_key='your_secret_key'\n",
    "    )\n",
    "    s3.upload_file('/path/to/exported_data.csv', 'your_bucket_name', 'exported_data.csv')\n",
    "\n",
    "upload_to_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b22b1-e039-46fa-a1cd-756761c3a530",
   "metadata": {},
   "source": [
    "## Load Data from s3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d03574-fe26-4541-8512-a910f71a78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY your_redshift_table\n",
    "FROM 's3://your_bucket_name/exported_data.csv'\n",
    "IAM_ROLE 'arn:aws:iam::your_account_id:role/your_redshift_role'\n",
    "CSV\n",
    "IGNOREHEADER 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e71fa7-cf13-48b0-bb3a-325f67beb889",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226944a-b677-419a-9d81-180047b491f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.hooks.redshift import RedshiftHook\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "default_args = {'start_date': datetime(2025, 3, 22)}\n",
    "dag = DAG('postgres_to_redshift', default_args=default_args, schedule_interval='@daily')\n",
    "\n",
    "# Task 1: Export PostgreSQL Data\n",
    "export_task = PythonOperator(\n",
    "    task_id='export_postgres_to_csv',\n",
    "    python_callable=export_data_to_csv,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Upload CSV to S3\n",
    "upload_task = PythonOperator(\n",
    "    task_id='upload_csv_to_s3',\n",
    "    python_callable=upload_to_s3,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Load to Redshift\n",
    "redshift_task = PostgresOperator(\n",
    "    task_id='load_to_redshift',\n",
    "    postgres_conn_id='redshift_default',\n",
    "    sql=\"\"\"\n",
    "        COPY your_redshift_table\n",
    "        FROM 's3://your_bucket_name/exported_data.csv'\n",
    "        IAM_ROLE 'arn:aws:iam::your_account_id:role/your_redshift_role'\n",
    "        CSV\n",
    "        IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define Task Dependencies\n",
    "export_task >> upload_task >> redshift_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cae3c-0e21-40ba-b045-bbcfc590253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41ddab-da9f-4877-897f-687ec80dd863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37600d95-f743-4483-ad8c-b6bf6a11ed3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9cfa5-20d1-4b3d-954d-ac728eb1066e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
