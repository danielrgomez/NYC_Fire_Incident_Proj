{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e632b788-7561-4ede-9dbf-c13eb8bdcfc5",
   "metadata": {},
   "source": [
    "## Extract from Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13681c-5581-4efc-b239-844cd113523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import csv\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68505c31-37c8-430f-89c1-c3311d8cfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data_to_csv():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname='fire_incidents_db',\n",
    "        user='root',\n",
    "        password='root',\n",
    "        host='fire_incidents_db_container',\n",
    "        port=5432\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM fire_incidents_tbl\")\n",
    "    \n",
    "    with open('./temp_csv_files/exported_nyc_fire_incidents_data.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i[0] for i in cursor.description])  # Write headers\n",
    "        writer.writerows(cursor.fetchall())  # Write data\n",
    "    \n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "export_data_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6cec2-b50e-4c03-9081-7d33994d3481",
   "metadata": {},
   "source": [
    "## Upload CSV File to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8fa5e3e-84cb-4771-8f4b-22563c46c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "##This may be the preferred approach as opposed to the above\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "# Assume the IAM role\n",
    "assumed_role = sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::564001313146:role/S3AccessRoleForNYCFireIncidentsProj\",\n",
    "    RoleSessionName=\"MyS3Session\"\n",
    ")\n",
    "\n",
    "# Extract temporary credentials\n",
    "credentials = assumed_role['Credentials']\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken']\n",
    ")\n",
    "\n",
    "# Upload a file\n",
    "s3_client.upload_file('./temp_csv_files/exported_nyc_fire_incidents_data.csv', 'nyc-fire-incidents-s3', 'exported_nyc_fire_incidents_data.csv')\n",
    "\n",
    "print(\"File uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b22b1-e039-46fa-a1cd-756761c3a530",
   "metadata": {},
   "source": [
    "## Load Data from s3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d03574-fe26-4541-8512-a910f71a78dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY your_redshift_table\n",
    "FROM 's3://your_bucket_name/exported_data.csv'\n",
    "IAM_ROLE 'arn:aws:iam::your_account_id:role/your_redshift_role'\n",
    "CSV\n",
    "IGNOREHEADER 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a172ad1-ff38-476c-b190-794738ce7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Define your Redshift credentials\n",
    "redshift_host = \"your-redshift-cluster-endpoint\"\n",
    "redshift_port = 5439  # Default Redshift port\n",
    "redshift_dbname = \"your_database_name\"\n",
    "redshift_user = \"your_username\"\n",
    "redshift_password = \"your_password\"\n",
    "\n",
    "# Define your COPY command parameters\n",
    "copy_command = \"\"\"\n",
    "COPY your_redshift_table\n",
    "FROM 's3://your_bucket_name/exported_data.csv'\n",
    "IAM_ROLE 'arn:aws:iam::your_account_id:role/your_redshift_role'\n",
    "CSV\n",
    "IGNOREHEADER 1;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Establish a connection to Redshift\n",
    "    connection = psycopg2.connect(\n",
    "        dbname=redshift_dbname,\n",
    "        user=redshift_user,\n",
    "        password=redshift_password,\n",
    "        host=redshift_host,\n",
    "        port=redshift_port\n",
    "    )\n",
    "    connection.autocommit = True  # Auto-commit for COPY command\n",
    "\n",
    "    # Create a cursor object to execute the query\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Execute the COPY command\n",
    "    cursor.execute(sql.SQL(copy_command))\n",
    "\n",
    "    print(\"COPY command executed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e71fa7-cf13-48b0-bb3a-325f67beb889",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226944a-b677-419a-9d81-180047b491f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.amazon.aws.hooks.redshift import RedshiftHook\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the DAG\n",
    "default_args = {'start_date': datetime(2025, 3, 22)}\n",
    "dag = DAG('postgres_to_redshift', default_args=default_args, schedule_interval='@daily')\n",
    "\n",
    "# Task 1: Export PostgreSQL Data\n",
    "export_task = PythonOperator(\n",
    "    task_id='export_postgres_to_csv',\n",
    "    python_callable=export_data_to_csv,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Upload CSV to S3\n",
    "upload_task = PythonOperator(\n",
    "    task_id='upload_csv_to_s3',\n",
    "    python_callable=upload_to_s3,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Load to Redshift\n",
    "redshift_task = PostgresOperator(\n",
    "    task_id='load_to_redshift',\n",
    "    postgres_conn_id='redshift_default',\n",
    "    sql=\"\"\"\n",
    "        COPY your_redshift_table\n",
    "        FROM 's3://your_bucket_name/exported_data.csv'\n",
    "        IAM_ROLE 'arn:aws:iam::your_account_id:role/your_redshift_role'\n",
    "        CSV\n",
    "        IGNOREHEADER 1;\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define Task Dependencies\n",
    "export_task >> upload_task >> redshift_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cae3c-0e21-40ba-b045-bbcfc590253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41ddab-da9f-4877-897f-687ec80dd863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37600d95-f743-4483-ad8c-b6bf6a11ed3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e9cfa5-20d1-4b3d-954d-ac728eb1066e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
