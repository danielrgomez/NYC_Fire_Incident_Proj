

#USER root
#
## Install any additional dependencies (non-Python)
#RUN apt-get update && apt-get install -y \
#    some-package \
#    && rm -rf /var/lib/apt/lists/*


#USER airflow
# Base Image
#FROM apache/airflow:2.10.5
#
## Switch to root to install system-level dependencies
#USER root
#
## Install Java (required for PySpark)
#RUN apt-get update && \
#    apt-get install -y default-jdk && \
#    apt-get clean && \
#    rm -rf /var/lib/apt/lists/*
#
## Set Java environment variables
#ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
#ENV PATH="${JAVA_HOME}/bin:${PATH}"
#
## Switch back to airflow user
#USER airflow
#
## Install PySpark
#RUN pip install --no-cache-dir pyspark




FROM apache/airflow:2.10.5

# Switch to root user for system-level installations
USER root

# Install Java (required by PySpark)
#RUN apt-get update && apt-get install -y openjdk-11-jdk && apt-get clean
RUN apt-get update && apt-get install -y default-jdk && apt-get clean

#
# Switch back to the airflow user
USER airflow

# Install PySpark
RUN pip install pyspark





#USED TO WORK
## Stage 2: Create Airflow image
#FROM apache/airflow:2.10.5
#
##USER root
#USER root
#
## Install Java
#RUN apt-get update && apt-get install -y default-jdk && apt-get clean
##RUN apt-get update && apt-get install -y openjdk-8-jdk && apt-get clean
#ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
#ENV PATH=$JAVA_HOME/bin:$PATH
#
#
##ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
##ENV PATH=$JAVA_HOME/bin:$PATH
#
#
#USER airflow
#
## Install PySpark
#RUN pip install pyspark
##
##USED TO WORK



#COPY requirements.txt .

# Install any other dependencies
#RUN pip install --no-cache-dir -r requirements.txt

# Install any additional dependencies (non-Python)
#RUN apt-get update && apt-get upgrade -y && apt-get install -y \
#    some-package \
#    && rm -rf /var/lib/apt/lists/*

# Copy your DAGs and other necessary files
#COPY dags/ /opt/airflow/dags/
#COPY requirements.txt /opt/airflow/dags/requirements.txt

